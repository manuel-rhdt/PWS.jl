var documenterSearchIndex = {"docs":
[{"location":"#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"","page":"Background","title":"Background","text":"Information is a fundamental resource in complex systems at all scales, from bacterial signaling networks to artificial neural networks.  These systems receive input signals which are analyzed, filtered, transcoded, or otherwise transformed to yield an output signal. The generic mathematical framework to study the flow of information in these systems is information theory which defines a central quantity to quantify the amount of information that the output contains about the input: the mutual information. Specifically, this measure quantifies the number of distinct mappings between input and output that can be distinguished uniquely and is thus a measure of the fidelity of the input-output relationship.","category":"page"},{"location":"","page":"Background","title":"Background","text":"Mathematically, the mutual information between two random variables, S and X is defined as $ \\mathrm{I}(S, X) = \\sum_{s\\in S, x\\in X} \\mathrm{P}(s, x) \\ln\\frac{\\mathrm{P}(s, x)}{\\mathrm{P}(s) \\mathrm{P}(x)} \\,. $","category":"page"},{"location":"","page":"Background","title":"Background","text":"However, the mutual information between scalar random variables S and X does not quantify the rate of information transmission. Indeed, most systems receive time-dependent input signals, i.e. a sequence of messages over time, which are transformed into a time-dependent output signal.  In most systems a given input message is not independent from previous input messages. These autocorrelations within the input reduce the rate at which information is received. Additionally, correlations within the output signal can also reduce the information transmission rate. Hence, to quantify the rate of information transmission, the instantaneous mutual information between scalar random variables S and X is not sufficient, and we require a better mutual information measure.","category":"page"},{"location":"","page":"Background","title":"Background","text":"The solution to this problem is to compute the mutual information between entire trajectories of input and output, not between input and output values at given time points. In this way, correlations within the input and the output are taken into account when computing the mutual information. The mathematical form of the trajectory mutual information is analogous to the scalar case: $ \\mathrm{I}(\\bm{S}, \\bm{X}) = \\sum{\\bm{s}\\in \\bm{S}, \\bm{x}\\in \\bm{X}} \\mathrm{P}(\\bm{s}, \\bm{x}) \\ln\\frac{\\mathrm{P}(\\bm{s}, \\bm{x})}{\\mathrm{P}(\\bm{s}) \\mathrm{P}(\\bm{x})} $ but now the sum runs over all possible input and output trajectories (which are denoted using bold symbols), not unlike a path integral.  The information transmission rate mathrmR(bmS bmX) is then defined as the asymptotic rate at which the trajectory mutual information increases with the duration of the input and output trajectories, i.e. $ \\mathrm{R}(\\bm{S}, \\bm{X}) = \\lim{T\\rightarrow\\infty} \\frac{\\mathrm{d}\\mathrm{I}(\\bm{S}T, \\bm{X}T)}{\\mathrm{d}T} $ where bmS_T and bmX_T are trajectory-valued random variables of trajectories with duration T.","category":"page"},{"location":"","page":"Background","title":"Background","text":"PWS is a method to compute the mutual information between input and output trajectories for systems described by a master equation. ","category":"page"},{"location":"thermodynamic_integration/#Thermodynamic-Integral","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"","category":"section"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"From the master equation we can compute mathrm P(mathbf s) and mathrm P(mathbf xmathbf s)\nthe mutual information rate is defined as the asymptotic increase of mutual information between trajectories of duration T.\nmathrm H(mathcal X) = intmathrm dmathbf x mathrm P(mathbf x)lnmathrm P(mathbf x)\nmathrm P(mathbf x)\ncan be regarded as the normalization constant for mathrm P(mathbf smathbf x)","category":"page"},{"location":"thermodynamic_integration/#Creating-a-Markov-Chain-of-Entire-Trajectories","page":"Thermodynamic Integral","title":"Creating a Markov Chain of Entire Trajectories","text":"","category":"section"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"mathrm P(mathbf smathbf x)","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"is a probability density defined for entire trajectories mathbf s = (s_1 t_1)ldots(s_Nt_N) and mathbf x = (x_1 t_1)ldots(x_Mt_M). Using the Metropolis acceptance criterion we can create a Markov chain for trajectories mathbf s with an arbitrary stationary distribution p_S(mathbf s), provided that we can compute a function f_S(mathbf s) that is proportional to p_S.","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"The goal of the Metropolis-Hastings algorithm is to build a chain of trajectories mathbf s_1ldotsmathbf s_K that are distributed according to p_S(mathbf s). A step in the Metropolis-Hastings algorithm involves generating a correlated trajectory mathbf s^prime from an arbitrarily chosen trial distribution mathrm P(mathbf s_nrightarrowmathbf s^prime). We accept the new trajectory mathbf s^prime with probability mathrm A(mathbf s^prime mathbf s_n) = minleft 1  fracf_S(mathbf s^prime)f_S(mathbf s) fracmathrm P(mathbf s^prime rightarrow mathbf s_n)mathrm P(mathbf s_nrightarrowmathbf s^prime) right  In the case of acceptance we set mathbf s_n+1 = mathbf s^prime and otherwise we just have mathbf s_n+1=mathbf s_n This acceptance criterion ensures that the chain converges towards the desired stationary distribution.","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"While the choice of the trial distribution mathrm P(mathbf s_nrightarrowmathbf s^prime) is in principle arbitrary","category":"page"}]
}
