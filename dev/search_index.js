var documenterSearchIndex = {"docs":
[{"location":"guide/#Guide","page":"Guide","title":"Guide","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"In this section we will show step by step how PathWeightSampling.jl can be used to compute the mutual information for a simple model of gene expression.","category":"page"},{"location":"guide/#Setting-Up-the-System","page":"Guide","title":"Setting Up the System","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The model considered in this example consists of four reactions:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"                κ\nreaction 1:  ∅ ---> S\n\n                λ\nreaction 2:  S ---> ∅\n\n                ρ\nreaction 3:  S ---> S + X\n\n                μ\nreaction 4:  X ---> ∅","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The first two reactions specify the evolution of the input signal S, and last two reactions specify the evolution of the output X. Thus, both the input and output signal are modeled as a simple birth-death process, however the birth rate of X increases with higher copy numbers of  S.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The first step is to create the system that we are going to use. The simple gene expression model shown above is already included as an example in PathWeightSampling.jl and can be directly used as follows:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using PathWeightSampling\n\nsystem = PathWeightSampling.gene_expression_system()","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The result is a system consisting of the 4 reactions mentioned above and default values for the initial condition and the parameters that specify the reaction rates.","category":"page"},{"location":"guide/#Generating-and-Plotting-Trajectories","page":"Guide","title":"Generating and Plotting Trajectories","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"We can generate a configuration of this system. A configuration is a combination of an input trajectory and an output trajectories. Using generate_configuration we can create a configuration by first simulating an input trajectory and then use that input trajectory to simulate a corresponding output trajectory.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"conf = generate_configuration(system)","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Let us plot the generated configuration:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"ENV[\"GKSwstype\"] = \"100\" # hide\nusing Plots\nplot(conf)\nsavefig(\"plot1.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"We see a plot of the generated input and output trajectories that make up the configuration.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The individual trajectories of the configuration can also be accessed directly:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"input_traj = conf.s_traj\noutput_traj = conf.x_traj\np1 = plot(input_traj, label=\"input\")\np2 = plot(output_traj, label=\"output\")\nplot(p1, p2, layout = (2, 1))\nsavefig(\"plot2.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/#Computing-the-Trajectory-Mutual-Information","page":"Guide","title":"Computing the Trajectory Mutual Information","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"For our system we can compute the trajectory mutual information straightforwardly. ","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"result = PathWeightSampling.mutual_information(system, DirectMCEstimate(256), num_samples=100)\nnothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"This performs a full PWS Monte Carlo simulation and displays a progress bar during the computation. Naturally, the PWS.mutual_information takes the system as its first argument. The second argument is an object specifying the marginalization algorithm to use for computing the marginal trajectory probability. Here we chose the simple brute-force DirectMC algorithm with M=256 samples. Thus, we compute a \"Direct PWS\" estimate. The final keyword argument is the overall number of Monte Carlo samples to use for estimating the mutual information. This is the number of samples taken in the outer Monte Carlo simulation as opposed to the M=256 samples taken in the inner Monte Carlo loop.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"result is a DataFrame containing the simulation results. We can display the individual Monte Carlo samples:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"plot(\n    system.dtimes, \n    result.MutualInformation, \n    color=:black, \n    linewidth=0.2, \n    legend=false, \n    xlabel=\"trajectory duration\", \n    ylabel=\"mutual information (nats)\"\n)\nsavefig(\"plot3.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The final Monte Carlo estimate is simply the mean of the individual samples:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using Statistics\nplot(\n    system.dtimes, \n    mean(result.MutualInformation), \n    color=:black, \n    linewidth=2, \n    legend=false,\n    xlabel=\"trajectory duration\",\n    ylabel=\"mutual information (nats)\"\n)\nsavefig(\"plot4.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Note that since we only used 100 MC samples the fluctuation of the result is relatively large. To judge the statistical error due to the number of Monte Carlo samples, we can additionally plot error bars. A common error measure in Monte Carlo simulations is the \"standard error of the mean\", defined as the standard deviation divided by the square root of the number of samples. We use this method to draw error bars.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"sem(x) = std(x) / sqrt(length(x))\nplot(\n    system.dtimes, \n    mean(result.MutualInformation),\n    yerr=sem(result.MutualInformation), \n    color=:black, \n    linewidth=2, \n    legend=false,\n    xlabel=\"trajectory duration\",\n    ylabel=\"mutual information (nats)\"\n)\nsavefig(\"plot5.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/#More-Advanced-Marginalization-Strategies","page":"Guide","title":"More Advanced Marginalization Strategies","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"So far we computed the mutual information using the brute-force Direct PWS algorithm. However, we can choose a different approach to perform the marginalization integrals. To change the marginalization strategy we simply pass a different algorithm as the second argument of PWS.mutual_information. The possible choices for the marginalization strategy are","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"DirectMCEstimate(m): The simple brute force marginalization using a Direct Monte Carlo estimate. The integer m specifies the number of samples to use per brute-force computation. This method works well for short trajectories but becomes exponentially worse for longer trajectories.\nSMCEstimate(m): Improved computation of marginalization integrals using a sequential Monte Carlo technique (specifically using a particle filter). The integer m specifies the number of \"particles\" that are being propagated simultaneously. This method works much better than the DirectMCEstimate for long trajectories.\nTIEstimate(burn_in, integration_nodes, num_samples): Use thermodynamic integration to compute the marginalization integrals. This will set up a number of MCMC simulations in path-space to perform the TI integral. burn_in specifies the number of initial samples from the MCMC simulation to be discarded, integration_nodes specifies the number of points to use in the Gaussian quadrature, and num_samples specifies the number of MCMC samples per integration node to generate.\nAnnealingEstimate(subsample, num_temps, num_samples): Use annealed importance sampling to compute the marginalization integrals. This technique is very similar to thermodynamic integration and also uses MCMC simulations in path space. subsample specifies the number of Metropolis trials to perform before recording a new MCMC sample. num_temps sets how many different \"temperatures\" should be used for the annealing. num_samples is the number of MCMC samples to use per temperature setting.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"We can compute the mutual information using each of these strategies and compare the results:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"strategies = [\n    DirectMCEstimate(128), \n    SMCEstimate(128), \n    TIEstimate(0, 8, 16), \n    # AnnealingEstimate(0, 128, 1)\n]\nresults = [PathWeightSampling.mutual_information(system, strat, num_samples=100, progress=false) for strat in strategies]\n\nplot()\nfor (strat, r) in zip(strategies, results)\n    plot!(\n        system.dtimes, \n        mean(r.MutualInformation),\n        label=PathWeightSampling.name(strat),\n        xlabel=\"trajectory duration\",\n        ylabel=\"mutual information (nats)\"\n    )\nend\n\nsavefig(\"plot6.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/#API-Summary","page":"Guide","title":"API Summary","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"Thus, the core function to estimate the trajectory mutual information is PathWeightSampling.mutual_information. A complete description of its arguments and return value is given below.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"PathWeightSampling.mutual_information","category":"page"},{"location":"guide/#PathWeightSampling.mutual_information","page":"Guide","title":"PathWeightSampling.mutual_information","text":"mutual_information(system, algorithm; num_samples=1, progress=true)\n\nPerform a simulation to compute the mutual information between input and output trajectories of system. \n\nArguments\n\nThe required marginalization integrals to obtain the marginal probability mathcalPbmx are performed using the specified algorithm.\n\nOverall, num_samples Monte Carlo samples are performed. For each individual sample, one or mupltiple marginalization operations need to be performed.\n\nIf progress == true, a progress bar will be shown during the computation.\n\nReturns\n\nReturns a DataFrame containing the results of the simulation. This resulting DataFrame has 3 columns. Assuming, the returned value has been named result the columns can be accessed by:\n\nresult.MutualInformation: A vector of vectors that contains the results of the simulation. Each element of the outer vector is the result of a single Monte Carlo sample. Each element is a vector containing the trajectory mutual information estimates for each time specified in system.dtimes.\nresult.TimeMarginal: A vector containing, for each sample, the CPU time in seconds used for the computation of the marginal entropy.\nresult.TimeConditional: A vector containing, for each sample, the CPU time in seconds used for the computation of the conditional entropy.\n\n\n\n\n\n","category":"function"},{"location":"systems/#Systems","page":"Systems","title":"Systems","text":"","category":"section"},{"location":"systems/#Example-Systems-included-in-PathWeightSampling.jl","page":"Systems","title":"Example Systems included in PathWeightSampling.jl","text":"","category":"section"},{"location":"systems/","page":"Systems","title":"Systems","text":"PathWeightSampling.gene_expression_system\nPathWeightSampling.cooperative_chemotaxis_system","category":"page"},{"location":"systems/#PathWeightSampling.gene_expression_system","page":"Systems","title":"PathWeightSampling.gene_expression_system","text":"gene_expression_system(; mean_s=50, mean_x=mean_s, corr_time_s=1.0, corr_time_x=0.1, u0=SA[mean_s, mean_x], dtimes=0:0.1:2.0)\n\nCreates a system for a very simple model of gene expression.\n\nModel Description\n\nThis model consists of four reactions:\n\n                κ\nreaction 1:  ∅ ---> S\n\n                λ\nreaction 2:  S ---> ∅\n\n                ρ\nreaction 3:  S ---> S + X\n\n                μ\nreaction 4:  X ---> ∅\n\nThe first two reactions specify the evolution of the input signal S, and last two reactions specify the evolution of the output X. Thus, both the input and output signal are modeled as a simple birth-death  process, however the birth rate of X increases with higher copy numbers  of  S.\n\nExamples\n\nThe values of the reaction rates can be specified directly as follows:\n\nusing PathWeightSampling\nsystem = PathWeightSampling.gene_expression_system(kappa = 10.0, lambda = 0.1, rho = 1.0, mu = 1.0)\n\n# output\n\nSimpleSystem with 4 reactions\nInput variables: S(t)\nOutput variables: X(t)\nInitial condition:\n    S(t) = 50\n    X(t) = 50\nParameters:\n    κ = 10.0\n    λ = 0.1\n    ρ = 1.0\n    μ = 1.0\n\nAlternatively, the reaction rates can be specified indirectly through the following arguments:\n\nmean_s: The average copy number of S when the system has relaxed to steady state.\nmean_x: The average copy number of X when the system has relaxed to steady state.\ncorr_time_s: The input signal correlation time. The shorter this time is, the faster the fluctuations in the input signal.\ncorr_time_x: The output signal correlation time. This sets the timescale of output fluctuations.\n\nusing PathWeightSampling\nsystem = PathWeightSampling.gene_expression_system(mean_s=25, mean_x=50, corr_time_s=1.0, corr_time_x=0.75)\n\n# output\n\n\n\n\n\n\n\n","category":"function"},{"location":"systems/#Create-a-New-System","page":"Systems","title":"Create a New System","text":"","category":"section"},{"location":"systems/","page":"Systems","title":"Systems","text":"To create a new system, you can make either an object of type SimpleSystem or ComplexSystem from scratch. The difference between the two is that a  ComplexSystem represents a model that includes latent variables which need to be integrated out to compute the mutual information. A SimpleSystem represents a model without any latent variables.","category":"page"},{"location":"systems/","page":"Systems","title":"Systems","text":"Both types of systems are constructed using Catalyst's representation of a reaction network (i.e. from objects of type ReactionSystem). Those reaction systems can be constructed using the API from Catalyst.jl.","category":"page"},{"location":"systems/","page":"Systems","title":"Systems","text":"PathWeightSampling.SimpleSystem\nPathWeightSampling.ComplexSystem","category":"page"},{"location":"marginalization/#Marginalization-Strategies","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"The computationally most demanding part of PWS is the evaluation of the marginalization integral mathcalPbmx=intmathcalDbms mathcalPbmsbmx which needs to be computed repeatedly for many different outputs bmx_1ldotsbmx_N. Consequently the computational efficiency of the marginalization is essential for the overall simulation performance.","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"Marginalization is a general term to denote an operation where one or more variables are integrated out of a joint probability distribution, say mathcalPbmsbmx, to obtain the corresponding marginal probability distribution mathcalPbmx:","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalPbmx = intmathcalDbms mathcalPbmsbmx","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"Note that in our case, bms and bmx are trajectories and that therefore the integral intmathcalDbms is actually a path integral.","category":"page"},{"location":"marginalization/#Equivalence-of-Marginalization-Integrals-and-Free-Energy-Computations","page":"Marginalization Strategies","title":"Equivalence of Marginalization Integrals and Free Energy Computations","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"In this section we establish the equivalence between computing a marginalization integral and computing the free energy in statistical physics.","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"In the language of statistical physics, mathcalPbmx corresponds to the normalization constant, or partition function, of a Boltzmann distribution for the potential","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalUbmsbmx = -lnmathcalPbmsbmx ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"We interpret bms as a variable in the configuration space whereas bmx is an auxiliary variable, i.e. a parameter. Note that both bms and bmx still represent trajectories. For this potential, the partition function is given by","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalZbmx = intmathcalDbms e^-mathcalUbmsbmx ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"The integral only runs over the configuration space, i.e. we integrate only with respect to bms but not bmx, which remains a parameter of the partition function. The partition function is precisely equal to the marginal probability of the output, i.e. mathcalZbmx = mathcalPbmx, as can be verified by inserting the expression for the mathcalUbmsbmx. Further, the free energy is defined as","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalFbmx = -ln mathcalZbmx = -ln mathcalPbmx","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"i.e. the computation of the free energy of the trajectory ensemble corresponding to mathcalUbms bmx is equivalent to the computation of (the logarithm of) the marginal probability mathcalPbmx.","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"Note that above we omitted any factors of k_mathrmBT that are typically used in physics since temperature is irrelevant here.  Also note that while the distribution exp(-mathcalUbmsbmx) looks like the equilibrium distribution of a canonical ensemble from statistical mechanics, this does not imply that the we can only study systems in thermal equilibrium. Thus, the notation introduced in this section is nothing else but a mathematical reformulation of the marginalization integral to make the analogy to statistical physics apparent and we assign no additional meaning of the potentials and free energies introduced here.","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"In statistical physics it is well known that the free energy cannot be directly measured from a single simulation. Instead, one estimates the free-energy difference","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    DeltamathcalFbmx = mathcalFbmx - mathcalF_0bmx = -ln fracmathcalZbmxmathcalZ_0bmx","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"between the system and a reference system with known free energy mathcalF_0bmx. The reference system is described by the potential mathcalU_0bms bmx with the corresponding partition function mathcalZ_0bmx.  In our case, a natural choice of reference potential is","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalU_0bmsbmx=-lnmathcalPbms","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"with  the corresponding partition function","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"        mathcalZ_0bmx=intmathcalDbms mathcalPbms=1","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"This means that since mathcalPbms is a normalized probability density function, the reference free energy is zero (mathcalF_0bmx=-lnmathcalZ_0bmx=0). Hence, for the above choice of reference system, the free-energy difference is","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    DeltamathcalFbmx= mathcalFbmx = -lnmathcalPbmx","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"Note that in our case the reference potential mathcalU_0bmsbmx=-lnmathcalPbms does not depend on the output trajectory bmx, i.e. mathcalU_0bmsbmxequivmathcalU_0bms. It describes a non-interacting version of our input-output system where the input trajectories evolve completely independently of the fixed output trajectory bmx. ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"What is the interaction between the output bmx and the input trajectory ensemble? We define the interaction potential DeltamathcalUbms bmx through","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalUbms bmx = mathcalU_0bms + DeltamathcalUbms bmx ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"The interaction potential makes it apparent that the distribution of bms trajectories corresponding to the potential mathcalUbms bmx is biased by bmx with respect to the distribution corresponding to the reference potential mathcalU_0bms. By inserting the expressions for mathcalU_0bms and mathcalUbms bmx we see that","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    DeltamathcalUbms bmx = -lnmathcalPbmxbms ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"This expression illustrates that the interaction of the output trajectory bmx with the ensemble of input trajectories is characterized by the trajectory likelihood mathcalPbmxbms. Since we can compute the trajectory likelihood from the master equation, so can we compute the interaction potential.","category":"page"},{"location":"marginalization/#Direct-PWS","page":"Marginalization Strategies","title":"Direct PWS","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"DirectMCEstimate","category":"page"},{"location":"marginalization/#PathWeightSampling.DirectMC.DirectMCEstimate","page":"Marginalization Strategies","title":"PathWeightSampling.DirectMC.DirectMCEstimate","text":"DirectMCEstimate(M::Int)\n\nThis is the simplest marginalization strategy, estimating the marginal path probabilities using a direct Monte Carlo simulation.\n\nM specifies the number of samples to be used for the Monte Carlo average. Larger M improves the accuracy of the marginalization integral, but increases the computational cost.\n\nMathematical Description\n\nThe marginal probability\n\nmathrmPbmx = intmathrmdbms mathrmPbmxbms mathrmPbms\n\ncan be computed via a Monte Carlo estimate by sampling M trajectories from mathrmPbms and taking the average of the likelihoods:\n\nmathrmPbmx = langle mathrmPbmxbms rangle_mathrmPbms\n\n\n\n\n\n","category":"type"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"The direct scheme makes it possible to compute the marginal probability mathcalPbmx for the output trajectory of an externally driven Markov jump process. Yet, due to the combinatorial explosion of possible input trajectories, the variance of the direct scheme increases exponentially with trajectory length. Hence, for complex information processing networks and long trajectories the direct estimate may incur very high computational cost. Therefore, we implemented two improved variants of PWS which allow us to study more complex information processing networks.","category":"page"},{"location":"marginalization/#RR-PWS","page":"Marginalization Strategies","title":"RR-PWS","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"In Rosenbluth-Rosenbluth PWS (RR-PWS) we compute the free-energy difference DeltamathcalF between the ideal system mathcalU_0 and mathcalU in a single simulation just like with the direct method. However, instead of generating bms trajectories in an uncorrelated fashion according to exp(-mathcalU_0bms)=mathcalPbms, we bias our sampling distribution towards exp(-mathcalUbms bmx)proptomathcalPbmsbmx to reduce the sampling problems found in the DirectMCEstimate.","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"SMCEstimate","category":"page"},{"location":"marginalization/#PathWeightSampling.SMC.SMCEstimate","page":"Marginalization Strategies","title":"PathWeightSampling.SMC.SMCEstimate","text":"SMCEstimate(num_particles::Int)\n\nCompute the marginal trajectory probability using a Sequential Monte Carlo (SMC) algorithm, more specifically using a particle filter.\n\nIn a particle filter, num_particles trajectories are propagated in parallel. At regular intervals, the current set of parallel trajectories undergoes a resampling step where some of the trajectories get eliminated and others duplicated, depending on their accumulated likelihood. This resampling ensures that we don't sample trajectories that contribute only  very little to the marginalization integral.\n\nDue to the periodic resampling of the trajectories, this method works much better than the  DirectMCEstimate for long trajectories.\n\n\n\n\n\n","category":"type"},{"location":"marginalization/#TI-PWS","page":"Marginalization Strategies","title":"TI-PWS","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"The third marginalization scheme is based on the analogy of marginalization integrals with reversible work computations. As before, we view the problem of computing the marginal probability mathcalPbmx as equivalent to that of computing the free energy between ensembles that are defined by the potentials mathcalU_0bms bmx and mathcalUbms bmx, respectively. For the TI-PWS estimate we additionally assume that there is a continuous parameter that transforms the ensemble from mathcalU_0 to mathcalU.   Mathematically, we thus have a continuous mapping from thetain01 to a potential mathcalU_thetabmsbmx (where mathcalU=mathcalU_1) with a corresponding partition function ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"mathcalZ_thetabmx=intmathcalDbms e^-mathcalU_thetabmsbmx ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"For instance, for 0leqthetaleq 1, we can define our potential to be ","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalU_thetabmsbmx=mathcalU_0bms bmx+thetaDeltamathcalUbmsbmx","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"such that e^-mathcalU_thetabmsbmx=mathcalPbmsmathcalPbmxbms^theta.","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"To derive the thermodynamic integration estimate for the free-energy difference, we first compute the derivative of lnmathcalZ_thetabmx with respect to theta:","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"beginaligned\n    fracpartialpartial theta lnmathcalZ_thetabmx = frac1mathcalZ_thetabmx fracpartialpartial theta intmathcalDbms  e^-mathcalU_thetabmsbmx \n    = -leftlangle fracpartial mathcalU_thetabmsbmxpartialtheta rightrangle_theta\n    = -leftlangle\n    DeltamathcalUbmsbmx\n    rightrangle_theta\nendaligned","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"Thus, the derivative of lnmathcalZ_thetabmx is an average of the Boltzmann weight with respect to mathcalP_thetabmsbmx which is the ensemble distribution of bms given by","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    mathcalP_thetabmsbmx = frac1mathcalZ_thetabmx e^-mathcalU_thetabms bmx","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"Integrating with respect to theta leads to the formula for the free-energy difference","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"    DeltamathcalFbmx = -int^1_0 mathrmdtheta leftlangle \n    DeltamathcalUbms bmx\n    rightrangle_theta","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"which is the fundamental identity underlying thermodynamic integration.","category":"page"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"TIEstimate","category":"page"},{"location":"marginalization/#PathWeightSampling.ThermodynamicIntegration.TIEstimate","page":"Marginalization Strategies","title":"PathWeightSampling.ThermodynamicIntegration.TIEstimate","text":"TIEstimate(burn_in::Int, integration_nodes::Int, num_samples::Int)\n\nCompute marginalization integrals using thermodynamic integration.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Simple-Parallel-Simulation","page":"Examples","title":"Simple Parallel Simulation","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The script below reads in a JSON file as its first argument that sets up the simulation parameters. The input file could for example look as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"{   \n    \"params\": {\n        \"duration\": 10.0,\n        \"run_name\": \"test\",\n        \"algorithm\": \"directmc\",\n        \"num_samples\": 500,\n        \"directmc_samples\": 500,\n        \"mean_s\": 50,\n        \"corr_time_s\": 1,\n        \"corr_time_ratio\": 10\n    }\n}","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Then the example script reads this input file, sets up the \"gene-expression\" system with the correct parameters, and performs a parallel PWS simulation. The results are stored in a HDF5 file, which is the input filename with the extension .hdf5 appended.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributed\n\n@everywhere begin\n    import JSON\n    using Logging\n    using HDF5\n    using PathWeightSampling\nend\n\nf = ARGS[1]\ndict = JSON.parsefile(f)[\"params\"]\n@info \"Read file\" file = f\n\nif !haskey(dict, \"dtimes\")\n    dict[\"dtimes\"] = collect(0.0:0.1:dict[\"duration\"])\nend\n\nif dict[\"algorithm\"] == \"ti\"\n    algorithm = TIEstimate(0, 16, dict[\"ti_samples\"])\nelseif dict[\"algorithm\"] == \"annealing\"\n    algorithm = AnnealingEstimate(5, 100, 100)\nelseif dict[\"algorithm\"] == \"directmc\"\n    algorithm = DirectMCEstimate(dict[\"directmc_samples\"])\nelseif dict[\"algorithm\"] == \"smc\"\n    algorithm = SMCEstimate(dict[\"smc_samples\"])\nelse\n    error(\"Unsupported algorithm \" * dict[\"algorithm\"])\nend\n\n@info \"Parameters\" dict\n@info \"Algorithm\" algorithm\n\nsystem_fn = () -> PathWeightSampling.gene_expression_system(\n    mean_s = dict[\"mean_s\"],\n    corr_time_s = dict[\"corr_time_s\"],\n    corr_time_x = dict[\"corr_time_s\"] / dict[\"corr_time_ratio\"],\n    dtimes = dict[\"dtimes\"]\n)\n\nmi = PathWeightSampling.run_parallel(system_fn, algorithm, dict[\"num_samples\"])\nresult = Dict(\n    \"Samples\" => mi, \n    \"Parameters\" => dict\n)\n\nfilename = f * \".hdf5\"\nh5open(filename, \"w\") do file\n    PathWeightSampling.write_hdf5!(file, result)\nend\n@info \"Saved to\" filename","category":"page"},{"location":"#PathWeightSampling.jl","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"","category":"section"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"PathWeightSampling.jl is a Julia package to compute information transmission rates using the Path Weight Sampling (PWS) method.","category":"page"},{"location":"#Background","page":"PathWeightSampling.jl","title":"Background","text":"","category":"section"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"Information is a fundamental resource in complex systems at all scales, from bacterial signaling networks to artificial neural networks.  These systems receive input signals which are analyzed, filtered, transcoded, or otherwise transformed to yield an output signal. The generic mathematical framework to study the flow of information in these systems is information theory which defines a central quantity to quantify the amount of information that the output contains about the input: the mutual information. Specifically, this measure quantifies the number of distinct mappings between input and output that can be distinguished uniquely and is thus a measure of the fidelity of the input-output relationship.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"Mathematically, the mutual information between two random variables, S and X is defined as","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"mathrmI(S X) = sum_sin S xin X mathrmP(s x) lnfracmathrmP(s x)mathrmP(s) mathrmP(x) ","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"However, the mutual information between scalar random variables S and X does not quantify the rate of information transmission. Indeed, most systems receive time-dependent input signals, i.e. a sequence of messages over time, which are transformed into a time-dependent output signal.  In most systems a given input message is not independent from previous input messages. These autocorrelations within the input reduce the rate at which information is received. Additionally, correlations within the output signal can also reduce the information transmission rate. Hence, to quantify the rate of information transmission, the instantaneous mutual information between scalar random variables S and X is not sufficient, and we require a better mutual information measure.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"The solution to this problem is to compute the mutual information between entire trajectories of input and output, not between input and output values at given time points. In this way, correlations within the input and the output are taken into account when computing the mutual information. The mathematical form of the trajectory mutual information is analogous to the scalar case:","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"mathrmI(bmS bmX) = sum_bmsin bmS bmxin bmX mathrmP(bms bmx) lnfracmathrmP(bms bmx)mathrmP(bms) mathrmP(bmx)","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"but now the sum runs over all possible input and output trajectories (which are denoted using bold symbols), not unlike a path integral.  The information transmission rate mathrmR(bmS bmX) is then defined as the asymptotic rate at which the trajectory mutual information increases with the duration of the input and output trajectories, i.e.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"mathrmR(bmS bmX) = lim_Trightarrowinfty fracmathrmdmathrmI(bmS_T bmX_T)mathrmdT","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"where bmS_T and bmX_T are trajectory-valued random variables of trajectories with duration T.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"PWS is a novel method to compute the mutual information between input and output trajectories for systems described by a master equation. ","category":"page"},{"location":"#Installation","page":"PathWeightSampling.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"From the Julia REPL, type","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"julia> import Pkg; Pkg.add(\"PathWeightSampling\")","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"Alternatively, you can install this package by starting Julia, typing ] and then","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"pkg> add PathWeightSampling","category":"page"},{"location":"#Quick-Start","page":"PathWeightSampling.jl","title":"Quick Start","text":"","category":"section"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"After installation, the package can be loaded from directly from julia.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"julia> using PathWeightSampling","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"We then need a system of reactions for which we want to compute the mutual information. We can use one of the included example systems, such as a simple model for gene expression.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"julia> system = PathWeightSampling.gene_expression_system()\nSimpleSystem with 4 reactions\nInput variables: S(t)\nOutput variables: X(t)\nInitial condition:\n    S(t) = 50\n    X(t) = 50\nParameters:\n    κ = 50.0\n    λ = 1.0\n    ρ = 10.0\n    μ = 10.0","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"This specific model is very simple, consisting of only 4 reactions:","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"∅ → S with rate κ\nS → ∅ with rate λ\nS → S + X with rate ρ\nX → ∅ with rate μ","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"S represents the input and X represents the output. The values of the parameters can be inspected from the output above. For this system, we can perform a PWS simulation to compute the mutual information between its input and output trajectories:","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"julia> result = mutual_information(system, DirectMCEstimate(256), num_samples=1000)","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"Here we just made a default choice for which marginalization algorithm to use. This computation takes approximately a minute on a typical laptop. The result  is a DataFrame with three columns and 1000 rows:","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"1000×3 DataFrame\n  Row │ TimeConditional  TimeMarginal  MutualInformation                 \n      │ Float64          Float64       Vector{Float64}                   \n──────┼──────────────────────────────────────────────────────────────────\n    1 │     0.000180898     0.0508378  [0.0, -0.67167, 0.388398, -0.343…\n  ⋮   │        ⋮              ⋮                        ⋮\n 1000 │     0.00020897      0.0694072  [0.0, 0.254173, 0.362607, 0.2584…\n                                                         998 rows omitted","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"Each row represents one Monte Carlo sample.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"TimeConditional is the CPU time in seconds for the computation of the conditional probability P(x|s)\nTimeMarginal is the CPU time in seconds for the computation of the marginal probability P(x|s)\nMutualInformation is the resulting mutual information estimate. This is a vector for each sample giving the mutual information for trajectories of different durations. The durations to which these individual values correspond is given by","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"julia> system.dtimes\n0.0:0.1:2.0","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"So we computed the mutual information for trajectories of duration 0.0, 0.1, 0.2, ..., 2.0.","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"We can plot the results (assuming the package Plots.jl is installed):","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"julia> using Plots, Statistics\njulia> plot(\n           system.dtimes,\n           mean(result.MutualInformation),\n           legend=false,\n           xlabel=\"trajectory duration\",\n           ylabel=\"mutual information (nats)\"\n       )","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"(Image: Plot of the mutual information as a function of trajectory duration for the simple gene expression system.)","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"Here we plot mean(result.MutualInformation), i.e. we compute the average of our Monte Carlo samples, which is the PWS estimate for the mutual information.","category":"page"},{"location":"#Acknowledgments","page":"PathWeightSampling.jl","title":"Acknowledgments","text":"","category":"section"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"This work was performed at the research institute AMOLF. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement No. 885065) and was financially supported by the Dutch Research Council (NWO) through the “Building a Synthetic Cell (BaSyC)” Gravitation grant (024.003.019).","category":"page"},{"location":"","page":"PathWeightSampling.jl","title":"PathWeightSampling.jl","text":"(Image: Logo NWO) (Image: Logo AMOLF) (Image: Logo BaSyC)","category":"page"},{"location":"thermodynamic_integration/#Thermodynamic-Integral","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"","category":"section"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"From the master equation we can compute mathrm P(mathbf s) and mathrm P(mathbf xmathbf s)\nthe mutual information rate is defined as the asymptotic increase of mutual information between trajectories of duration T.\nmathrm H(mathcal X) = intmathrm dmathbf x mathrm P(mathbf x)lnmathrm P(mathbf x)\nmathrm P(mathbf x)\ncan be regarded as the normalization constant for mathrm P(mathbf smathbf x)","category":"page"},{"location":"thermodynamic_integration/#Creating-a-Markov-Chain-of-Entire-Trajectories","page":"Thermodynamic Integral","title":"Creating a Markov Chain of Entire Trajectories","text":"","category":"section"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"mathrm P(mathbf smathbf x)","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"is a probability density defined for entire trajectories mathbf s = (s_1 t_1)ldots(s_Nt_N) and mathbf x = (x_1 t_1)ldots(x_Mt_M). Using the Metropolis acceptance criterion we can create a Markov chain for trajectories mathbf s with an arbitrary stationary distribution p_S(mathbf s), provided that we can compute a function f_S(mathbf s) that is proportional to p_S.","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"The goal of the Metropolis-Hastings algorithm is to build a chain of trajectories mathbf s_1ldotsmathbf s_K that are distributed according to p_S(mathbf s). A step in the Metropolis-Hastings algorithm involves generating a correlated trajectory mathbf s^prime from an arbitrarily chosen trial distribution mathrm P(mathbf s_nrightarrowmathbf s^prime). We accept the new trajectory mathbf s^prime with probability mathrm A(mathbf s^prime mathbf s_n) = minleft 1  fracf_S(mathbf s^prime)f_S(mathbf s) fracmathrm P(mathbf s^prime rightarrow mathbf s_n)mathrm P(mathbf s_nrightarrowmathbf s^prime) right  In the case of acceptance we set mathbf s_n+1 = mathbf s^prime and otherwise we just have mathbf s_n+1=mathbf s_n This acceptance criterion ensures that the chain converges towards the desired stationary distribution.","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"While the choice of the trial distribution mathrm P(mathbf s_nrightarrowmathbf s^prime) is in principle arbitrary","category":"page"}]
}
