var documenterSearchIndex = {"docs":
[{"location":"guide/#Guide","page":"Guide","title":"Guide","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"In this section we will show step by step how PWS.jl can be used to compute the mutual information for a simple model of gene expression.","category":"page"},{"location":"guide/#Setting-Up-the-System","page":"Guide","title":"Setting Up the System","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The model considered in this example consists of four reactions:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"                κ\nreaction 1:  ∅ ---> S\n\n                λ\nreaction 2:  S ---> ∅\n\n                ρ\nreaction 3:  S ---> S + X\n\n                μ\nreaction 4:  X ---> ∅","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The first two reactions specify the evolution of the input signal S, and last two reactions specify the evolution of the output X. Thus, both the input and output signal are modeled as a simple birth-death process, however the birth rate of X increases with higher copy numbers of  S.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The first step is to create the system that we are going to use. The simple gene expression model shown above is already included as an example in PWS.jl and can be directly used as follows:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using PWS\n\nsystem = PWS.gene_expression_system()","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The result is a system consisting of the 4 reactions mentioned above and default values for the initial condition and the parameters that specify the reaction rates.","category":"page"},{"location":"guide/#Generating-and-Plotting-Trajectories","page":"Guide","title":"Generating and Plotting Trajectories","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"We can generate a configuration of this system. A configuration is a combination of an input trajectory and an output trajectories. Using generate_configuration we can create a configuration by first simulating an input trajectory and then use that input trajectory to simulate a corresponding output trajectory.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"conf = generate_configuration(system)","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Let us plot the generated configuration:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"ENV[\"GKSwstype\"] = \"100\" # hide\nusing Plots\nplot(conf)\nsavefig(\"plot1.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"We see a plot of the generated input and output trajectories that make up the configuration.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The individual trajectories of the configuration can also be accessed directly:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"input_traj = conf.s_traj\noutput_traj = conf.x_traj\np1 = plot(input_traj, label=\"input\")\np2 = plot(output_traj, label=\"output\")\nplot(p1, p2, layout = (2, 1))\nsavefig(\"plot2.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/#Computing-the-Trajectory-Mutual-Information","page":"Guide","title":"Computing the Trajectory Mutual Information","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"For our system we can compute the trajectory mutual information straightforwardly. ","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"result = PWS.mutual_information(system, DirectMCEstimate(256), num_samples=100)\nnothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"This performs a full PWS Monte Carlo simulation and displays a progress bar during the computation. Naturally, the PWS.mutual_information takes the system as its first argument. The second argument is an object specifying the marginalization algorithm to use for computing the marginal trajectory probability. Here we chose the simple brute-force DirectMC algorithm with M=256 samples. Thus, we compute a \"Direct PWS\" estimate. The final keyword argument is the overall number of Monte Carlo samples to use for estimating the mutual information. This is the number of samples taken in the outer Monte Carlo simulation as opposed to the M=256 samples taken in the inner Monte Carlo loop.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"result is a DataFrame containing the simulation results. We can display the individual Monte Carlo samples:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"plot(\n    system.dtimes, \n    result.MutualInformation, \n    color=:black, \n    linewidth=0.2, \n    legend=false, \n    xlabel=\"trajectory duration\", \n    ylabel=\"mutual information (nats)\"\n)\nsavefig(\"plot3.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The final Monte Carlo estimate is simply the mean of the individual samples:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using Statistics\nplot(\n    system.dtimes, \n    mean(result.MutualInformation), \n    color=:black, \n    linewidth=2, \n    legend=false,\n    xlabel=\"trajectory duration\",\n    ylabel=\"mutual information (nats)\"\n)\nsavefig(\"plot4.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"Note that since we only used 100 MC samples the fluctuation of the result is relatively large. To judge the statistical error due to the number of Monte Carlo samples, we can additionally plot error bars. A common error measure in Monte Carlo simulations is the \"standard error of the mean\", defined as the standard deviation divided by the square root of the number of samples. We use this method to draw error bars.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"sem(x) = std(x) / sqrt(length(x))\nplot(\n    system.dtimes, \n    mean(result.MutualInformation),\n    yerr=sem(result.MutualInformation), \n    color=:black, \n    linewidth=2, \n    legend=false,\n    xlabel=\"trajectory duration\",\n    ylabel=\"mutual information (nats)\"\n)\nsavefig(\"plot5.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/#More-Advanced-Marginalization-Strategies","page":"Guide","title":"More Advanced Marginalization Strategies","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"So far we computed the mutual information using the brute-force Direct PWS algorithm. However, we can choose a different approach to perform the marginalization integrals. To change the marginalization strategy we simply pass a different algorithm as the second argument of PWS.mutual_information. The possible choices for the marginalization strategy are","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"DirectMCEstimate(m): The simple brute force marginalization using a Direct Monte Carlo estimate. The integer m specifies the number of samples to use per brute-force computation. This method works well for short trajectories but becomes exponentially worse for longer trajectories.\nSMCEstimate(m): Improved computation of marginalization integrals using a sequential Monte Carlo technique (specifically using a particle filter). The integer m specifies the number of \"particles\" that are being propagated simultaneously. This method works much better than the DirectMCEstimate for long trajectories.\nTIEstimate(burn_in, integration_nodes, num_samples): Use thermodynamic integration to compute the marginalization integrals. This will set up a number of MCMC simulations in path-space to perform the TI integral. burn_in specifies the number of initial samples from the MCMC simulation to be discarded, integration_nodes specifies the number of points to use in the Gaussian quadrature, and num_samples specifies the number of MCMC samples per integration node to generate.\nAnnealingEstimate(subsample, num_temps, num_samples): Use annealed importance sampling to compute the marginalization integrals. This technique is very similar to thermodynamic integration and also uses MCMC simulations in path space. subsample specifies the number of Metropolis trials to perform before recording a new MCMC sample. num_temps sets how many different \"temperatures\" should be used for the annealing. num_samples is the number of MCMC samples to use per temperature setting.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"We can compute the mutual information using each of these strategies and compare the results:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"strategies = [\n    DirectMCEstimate(128), \n    SMCEstimate(128), \n    TIEstimate(0, 8, 16), \n    # AnnealingEstimate(0, 128, 1)\n]\nresults = [PWS.mutual_information(system, strat, num_samples=100, progress=false) for strat in strategies]\n\nplot()\nfor (strat, r) in zip(strategies, results)\n    plot!(\n        system.dtimes, \n        mean(r.MutualInformation),\n        label=PWS.name(strat),\n        xlabel=\"trajectory duration\",\n        ylabel=\"mutual information (nats)\"\n    )\nend\n\nsavefig(\"plot6.svg\"); nothing # hide","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"(Image: )","category":"page"},{"location":"guide/#API-Summary","page":"Guide","title":"API Summary","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"Thus, the core function to estimate the trajectory mutual information is PWS.mutual_information. A complete description of its arguments and return value is given below.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"PWS.mutual_information","category":"page"},{"location":"guide/#PWS.mutual_information","page":"Guide","title":"PWS.mutual_information","text":"mutual_information(system, algorithm; num_samples=1, progress=true)\n\nPerform a simulation to compute the mutual information between input and output trajectories of system. \n\nArguments\n\nThe required marginalization integrals to obtain the marginal probability mathcalPbmx are performed using the specified algorithm.\n\nOverall, num_samples Monte Carlo samples are performed. For each individual sample, one or mupltiple marginalization operations need to be performed.\n\nIf progress == true, a progress bar will be shown during the computation.\n\nReturns\n\nReturns a DataFrame containing the results of the simulation. This resulting DataFrame has 3 columns. Assuming, the returned value has been named result the columns can be accessed by:\n\nresult.MutualInformation: A vector of vectors that contains the results of the simulation. Each element of the outer vector is the result of a single Monte Carlo sample. Each element is a vector containing the trajectory mutual information estimates for each time specified in system.dtimes.\nresult.TimeMarginal: A vector containing, for each sample, the CPU time in seconds used for the computation of the marginal entropy.\nresult.TimeConditional: A vector containing, for each sample, the CPU time in seconds used for the computation of the conditional entropy.\n\n\n\n\n\n","category":"function"},{"location":"marginalization/#Marginalization-Strategies","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"","category":"section"},{"location":"marginalization/#Direct-PWS","page":"Marginalization Strategies","title":"Direct PWS","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"DirectMCEstimate","category":"page"},{"location":"marginalization/#PWS.DirectMCEstimate","page":"Marginalization Strategies","title":"PWS.DirectMCEstimate","text":"DirectMCEstimate(M::Int)\n\nThis is the simplest marginalization strategy, estimating the marginal path probabilities using a direct Monte Carlo simulation.\n\nM specifies the number of samples to be used for the Monte Carlo average. Larger M improves the accuracy of the marginalization integral, but increases the computational cost.\n\nMathematical Description\n\nThe marginal probability\n\nmathrmPbmx = intmathrmdbms mathrmPbmxbms mathrmPbms\n\ncan be computed via a Monte Carlo estimate by sampling M trajectories from mathrmPbms and taking the average of the likelihoods:\n\nmathrmPbmx = langle mathrmPbmxbms rangle_mathrmPbms\n\n\n\n\n\n","category":"type"},{"location":"marginalization/#RR-PWS","page":"Marginalization Strategies","title":"RR-PWS","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"SMCEstimate","category":"page"},{"location":"marginalization/#PWS.SMCEstimate","page":"Marginalization Strategies","title":"PWS.SMCEstimate","text":"SMCEstimate(num_particles::Int)\n\nCompute the marginal trajectory probability using a Sequential Monte Carlo (SMC) algorithm, more specifically using a particle filter.\n\nIn a particle filter, num_particles trajectories are propagated in parallel. At regular intervals, the current set of parallel trajectories undergoes a resampling step where some of the trajectories get eliminated and others duplicated, depending on their accumulated likelihood. This resampling ensures that we don't sample trajectories that contribute only  very little to the marginalization integral.\n\nDue to the periodic resampling of the trajectories, this method works much better than the  DirectMCEstimate for long trajectories.\n\n\n\n\n\n","category":"type"},{"location":"marginalization/#TI-PWS","page":"Marginalization Strategies","title":"TI-PWS","text":"","category":"section"},{"location":"marginalization/","page":"Marginalization Strategies","title":"Marginalization Strategies","text":"TIEstimate","category":"page"},{"location":"marginalization/#PWS.TIEstimate","page":"Marginalization Strategies","title":"PWS.TIEstimate","text":"TIEstimate(burn_in::Int, integration_nodes::Int, num_samples::Int)\n\nCompute marginalization integrals using thermodynamic integration.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Simple-Parallel-Simulation","page":"Examples","title":"Simple Parallel Simulation","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The script below reads in a JSON file as its first argument that sets up the simulation parameters. The input file could for example look as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"{   \n    \"params\": {\n        \"duration\": 10.0,\n        \"run_name\": \"test\",\n        \"algorithm\": \"directmc\",\n        \"num_samples\": 500,\n        \"directmc_samples\": 500,\n        \"mean_s\": 50,\n        \"corr_time_s\": 1,\n        \"corr_time_ratio\": 10\n    }\n}","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Then the example script reads this input file, sets up the \"gene-expression\" system with the correct parameters, and performs a parallel PWS simulation. The results are stored in a HDF5 file, which is the input filename with the extension .hdf5 appended.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Distributed\n\n@everywhere begin\n    import JSON\n    using Logging\n    using HDF5\n    using PWS\nend\n\nf = ARGS[1]\ndict = JSON.parsefile(f)[\"params\"]\n@info \"Read file\" file = f\n\nif !haskey(dict, \"dtimes\")\n    dict[\"dtimes\"] = collect(0.0:0.1:dict[\"duration\"])\nend\n\nif dict[\"algorithm\"] == \"ti\"\n    algorithm = TIEstimate(0, 16, dict[\"ti_samples\"])\nelseif dict[\"algorithm\"] == \"annealing\"\n    algorithm = AnnealingEstimate(5, 100, 100)\nelseif dict[\"algorithm\"] == \"directmc\"\n    algorithm = DirectMCEstimate(dict[\"directmc_samples\"])\nelseif dict[\"algorithm\"] == \"smc\"\n    algorithm = SMCEstimate(dict[\"smc_samples\"])\nelse\n    error(\"Unsupported algorithm \" * dict[\"algorithm\"])\nend\n\n@info \"Parameters\" dict\n@info \"Algorithm\" algorithm\n\nsystem_fn = () -> PWS.gene_expression_system(\n    mean_s = dict[\"mean_s\"],\n    corr_time_s = dict[\"corr_time_s\"],\n    corr_time_x = dict[\"corr_time_s\"] / dict[\"corr_time_ratio\"],\n    dtimes = dict[\"dtimes\"]\n)\n\nmi = PWS.run_parallel(system_fn, algorithm, dict[\"num_samples\"])\nresult = Dict(\n    \"Samples\" => mi, \n    \"Parameters\" => dict\n)\n\nfilename = f * \".hdf5\"\nh5open(filename, \"w\") do file\n    PWS.write_hdf5!(file, result)\nend\n@info \"Saved to\" filename","category":"page"},{"location":"#PWS.jl","page":"PWS.jl","title":"PWS.jl","text":"","category":"section"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"PWS.jl is a Julia package to compute information transmission rates using the Path Weight Sampling (PWS) method.","category":"page"},{"location":"#Background","page":"PWS.jl","title":"Background","text":"","category":"section"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"Information is a fundamental resource in complex systems at all scales, from bacterial signaling networks to artificial neural networks.  These systems receive input signals which are analyzed, filtered, transcoded, or otherwise transformed to yield an output signal. The generic mathematical framework to study the flow of information in these systems is information theory which defines a central quantity to quantify the amount of information that the output contains about the input: the mutual information. Specifically, this measure quantifies the number of distinct mappings between input and output that can be distinguished uniquely and is thus a measure of the fidelity of the input-output relationship.","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"Mathematically, the mutual information between two random variables, S and X is defined as","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"mathrmI(S X) = sum_sin S xin X mathrmP(s x) lnfracmathrmP(s x)mathrmP(s) mathrmP(x) ","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"However, the mutual information between scalar random variables S and X does not quantify the rate of information transmission. Indeed, most systems receive time-dependent input signals, i.e. a sequence of messages over time, which are transformed into a time-dependent output signal.  In most systems a given input message is not independent from previous input messages. These autocorrelations within the input reduce the rate at which information is received. Additionally, correlations within the output signal can also reduce the information transmission rate. Hence, to quantify the rate of information transmission, the instantaneous mutual information between scalar random variables S and X is not sufficient, and we require a better mutual information measure.","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"The solution to this problem is to compute the mutual information between entire trajectories of input and output, not between input and output values at given time points. In this way, correlations within the input and the output are taken into account when computing the mutual information. The mathematical form of the trajectory mutual information is analogous to the scalar case:","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"mathrmI(bmS bmX) = sum_bmsin bmS bmxin bmX mathrmP(bms bmx) lnfracmathrmP(bms bmx)mathrmP(bms) mathrmP(bmx)","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"but now the sum runs over all possible input and output trajectories (which are denoted using bold symbols), not unlike a path integral.  The information transmission rate mathrmR(bmS bmX) is then defined as the asymptotic rate at which the trajectory mutual information increases with the duration of the input and output trajectories, i.e.","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"mathrmR(bmS bmX) = lim_Trightarrowinfty fracmathrmdmathrmI(bmS_T bmX_T)mathrmdT","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"where bmS_T and bmX_T are trajectory-valued random variables of trajectories with duration T.","category":"page"},{"location":"","page":"PWS.jl","title":"PWS.jl","text":"PWS is a novel method to compute the mutual information between input and output trajectories for systems described by a master equation. ","category":"page"},{"location":"thermodynamic_integration/#Thermodynamic-Integral","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"","category":"section"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"From the master equation we can compute mathrm P(mathbf s) and mathrm P(mathbf xmathbf s)\nthe mutual information rate is defined as the asymptotic increase of mutual information between trajectories of duration T.\nmathrm H(mathcal X) = intmathrm dmathbf x mathrm P(mathbf x)lnmathrm P(mathbf x)\nmathrm P(mathbf x)\ncan be regarded as the normalization constant for mathrm P(mathbf smathbf x)","category":"page"},{"location":"thermodynamic_integration/#Creating-a-Markov-Chain-of-Entire-Trajectories","page":"Thermodynamic Integral","title":"Creating a Markov Chain of Entire Trajectories","text":"","category":"section"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"mathrm P(mathbf smathbf x)","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"is a probability density defined for entire trajectories mathbf s = (s_1 t_1)ldots(s_Nt_N) and mathbf x = (x_1 t_1)ldots(x_Mt_M). Using the Metropolis acceptance criterion we can create a Markov chain for trajectories mathbf s with an arbitrary stationary distribution p_S(mathbf s), provided that we can compute a function f_S(mathbf s) that is proportional to p_S.","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"The goal of the Metropolis-Hastings algorithm is to build a chain of trajectories mathbf s_1ldotsmathbf s_K that are distributed according to p_S(mathbf s). A step in the Metropolis-Hastings algorithm involves generating a correlated trajectory mathbf s^prime from an arbitrarily chosen trial distribution mathrm P(mathbf s_nrightarrowmathbf s^prime). We accept the new trajectory mathbf s^prime with probability mathrm A(mathbf s^prime mathbf s_n) = minleft 1  fracf_S(mathbf s^prime)f_S(mathbf s) fracmathrm P(mathbf s^prime rightarrow mathbf s_n)mathrm P(mathbf s_nrightarrowmathbf s^prime) right  In the case of acceptance we set mathbf s_n+1 = mathbf s^prime and otherwise we just have mathbf s_n+1=mathbf s_n This acceptance criterion ensures that the chain converges towards the desired stationary distribution.","category":"page"},{"location":"thermodynamic_integration/","page":"Thermodynamic Integral","title":"Thermodynamic Integral","text":"While the choice of the trial distribution mathrm P(mathbf s_nrightarrowmathbf s^prime) is in principle arbitrary","category":"page"}]
}
